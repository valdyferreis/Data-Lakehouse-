{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Data Lakehouse with AWS Lake Formation and Apache Iceberg\n",
    "\n",
    "In this lab, you will create a Data Lakehouse with a medallion architecture using LakeFormation and Apache Iceberg tables. Your source system will be a \n",
    "relational database instantiated as MySQL database in Amazon RDS (Relational Database Service) and a streaming service that stores data in Amazon S3. You will first populate the medallion architecture with AWS Glue jobs, using Apache Iceberg tables for certain tables. You will finally query the data stored in the latest layer using Amazon Athena. To define and configure the components of this data pipeline example, you will use Terraform as Infrastructure as Code (IaC) service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "- [ 1 - Introduction and Setup](#1)\n",
    "  - [ 1.1 - Introduction](#1-1)\n",
    "  - [ 1.2 - Setting up the Data Lakehouse](#1-2)\n",
    "- [ 2 - Architecture of the Data Lakehouse](#2)\n",
    "- [ 3 - Landing Zone ](#3)\n",
    "  - [ 3.1 - RDS and Streaming Landing](#3-1)\n",
    "  - [ 3.2 - Deployment](#3-2)\n",
    "- [ 4 - Curated Zone](#4)\n",
    "  - [ 4.1 - CSV Transformation](#4-1)\n",
    "  - [ 4.2 - JSON Transformation and Apache Iceberg](#4-2)\n",
    "  - [ 4.3 - Deployment](#4-3)\n",
    "- [ 5 - Presentation zone](#5)\n",
    "- [ 6 - [Optional and Not Graded] - Some Features of Iceberg Format](#6)\n",
    "  - [ 6.1 - Schema Evolution](#6-1)\n",
    "  - [ 6.2 - Versioning with Iceberg](#6-2)\n",
    "- [ 7 - [Optional and Not Graded] - Granting Permissions with Lake Formation](#7)\n",
    "- [ 8 - Enviroment Clean Up](#8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import some libraries that you need for the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import boto3\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import awswrangler as wr\n",
    "from IPython.display import HTML\n",
    "from scripts import lf_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "## 1 - Introduction and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1-1'></a>\n",
    "### 1.1 - Introduction\n",
    "\n",
    "Assume you work as a Data Engineer at a retailer of scale models of classic cars and other transportation media. The retailer stores its historical purchases and its \n",
    "customers' information in a relational database that consists of the following tables: customers, products, productlines, orders, orderdetails, payments, employees, offices. You will use the same database example as in some previous labs: [MySQL Sample Database](https://www.mysqltutorial.org/mysql-sample-database.aspx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new web page of the company had a streaming service setup to quickly retrieve the ratings that users give to different products, the company has decided to migrate data architecture to the cloud. The relational database was recreated in RDS and the setup of a streaming sink to output the data was done in S3 bucket. You need to set up a Data Lakehouse, using medallion architecture to organize the data in three different zones:\n",
    "\n",
    "- **Landing**: Zone where raw data arrives from the RDS and the streaming service.\n",
    "- **Curated**: The raw data is enriched, curated and added to the Glue Data Catalog.\n",
    "- **Presentation**: Business objects are built on top of the curated data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1-2'></a>\n",
    "### 1.2 - Setting up the Data Lakehouse\n",
    "\n",
    "The Data Lake will be based on the S3 bucket, the name of the bucket is one of the outputs in CloudFormation (_Data Lake Bucket_). Your first job is to create folders to separate the bucket by the three zones:\n",
    "\n",
    "- `landing_zone`\n",
    "- `curated_zone`\n",
    "- `presentation_zone`\n",
    "\n",
    "Next, you will be using LakeFormation to manage the Data Lakehouse, most of the setup has been done to assign the admins of the Data Lakehouse. You will have to grant permission to the Glue Job role (`de-c3w2lab2-glue-role`) to be able to work inside the Data Lakehouse. A simple way to set up the permissions is using `boto3`. Start from the setup for the permissions using the first role."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2.1. Set up some boto3 clients and required variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lf_client = boto3.client('lakeformation', region_name='us-east-1')\n",
    "iam_client = boto3.client('iam', region_name='us-east-1')\n",
    "glue_client = boto3.client(\"glue\", region_name='us-east-1')\n",
    "\n",
    "AWS_ACCOUNT_ID = subprocess.run(['aws', 'sts', 'get-caller-identity', '--query', 'Account', '--output', 'text'], capture_output=True, text=True).stdout.strip()\n",
    "DATA_LAKE_BUCKET_NAME = f'de-c3w2a1-{AWS_ACCOUNT_ID}-us-east-1-data-lake'\n",
    "SCRIPTS_BUCKET_NAME = f'de-c3w2a1-{AWS_ACCOUNT_ID}-us-east-1-scripts'\n",
    "CURATED_DATABASE_NAME = 'curated_zone'\n",
    "PRESENTATION_DATABASE_NAME = 'presentation_zone'\n",
    "DATA_LOCATION_ARN = f\"arn:aws:s3:::{DATA_LAKE_BUCKET_NAME}\"\n",
    "VOCLABS_ARN = f'arn:aws:iam::{AWS_ACCOUNT_ID}:role/voclabs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2.2. You will need access to the AWS console. Run the following code to get the link.\n",
    "\n",
    "*Note*: For security reasons, the URL to access the AWS console will expire every 15 minutes, but any AWS resources you created will remain available for the 2 hour period. If you need to access the console after 15 minutes, please rerun this code cell to obtain a new active link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"https://signin.aws.amazon.com/federation?Action=login&SigninToken=sHt5rX3vytEDF_8bqp0hlpPIKqPdalQtJ9H4-lvzaiWAkF2Ht1dQD3yvwdQe5g8FFurii83FVwwrKMZasq1bm1CmDVcM2dZpa5iBU-ze-Gk4VgY96QilYh2soGtbSDcYm_O4BEKFqCv3ZY69wifzHiE5AXCF67wcC2Bsr_OTI_Ln9SlHZhAcXoZjQvY1lXrUKpg36DEAjNpju_WbYIoSh4GR5LvLcxZQ6ikJ83iylNE1iGQk4_vV99HNl13WNBE2vIhVZcCPqeiD0Wjz0BrIuRvNsyr3pW4UvF8kG0ZoZFicewU9A4d8LA3_eqPC-YVENU0SMUYzrqGRUa-SlWmGdSPpvr8a1SgznxQo_19qmjI9SwAfC3-vcSszL57i_GSxK1fNq5lYMrRNaSmUT5UR_2L3uJ6e7udEG2stlnDWb9Rcu-S7ktZWgg37w8Bray9o239VUtGxekF05C7eqBJ5jrx24gzAB4A7VAnvF8eliNgzBtWxYDQXRYrdiQk6zbly6xPSfBF6Dx7RSx3OTfPj6h1yvmBNB8qWCDG4Zso515mtoCzMNz89MufH0VrSGAANpk6lZAqkMGyPaPJ2XwvVYO9iettN2ZrIseVlFmxk8mSfv-mz4wnSnIMWpA58xTMKeQ4siCQzkFugyoVGVwuf3HMIAUk9Q2chgju9lOG9Vz7NfNzHKKP8ebReZRuUkv47pgvmBL9wiXGtxsc-C7ciUEN7oFrwkFUfkv2j8XVF-Ed-xMPvoqC4_91Mo1n-8QDUvThpcT7yKTjzWvfanNamzz4W3e4EfDnGa4WR3j7X2jkEIWWBhUpc5dlr9UCqwRQ8LDfK2ypwe3uCRMrIvuCQWu5Z2mdj0HZHPb7OAHrPW8titWQlKyzpLdT4MB5HTxouNbBL2fIa2D_oW8D56JsRAvSnC1jSSqVCdplzaYjl3WYt6Mpnb-szhaURCvkPq-Cg9V-Nzm9h-hHVhTBNgcAAIbpKtPCARUGhSz56b4uvpVxZIT_6tgH4aJaZt1UwTZknj7gxmcJG-vGo4sWb-7adnNhiVmgSqybjVdBNiVm6mBfLzSwEBsyLLW2b5-hCn3fMkI9oYHEOVKWTKqgwFJm5O28YcNy_qISrfcm4RhG7DMML5JhXuXhx10iujzoCgn3wlwLESdECj-qPZM9jwLBFLS3aZf6bEocLsjZ8ZDi-Y1FCUGKM9_6go7X5lwemdbB9y8rVLQVfpiMEAvG8X-cwfVKMpwCvTOLBA-Mevou4NOSo51XgVaFotO5OWvXS_UwxrSo5p1rYBVasvspv9xJQBYxOKhoJ0zBzV1daBXMlFi6UsU44vGjT-3vmfNIeUP8cMa7W_YdsU8aw8CHfCDfF70Ya8D8rcOvfLXAW6BaOTDK6PTfF1rFWhCZ3QaXUIN0z8wNgfNBUzydaWAUGizTA3bARzAgup8jI9ylZIQ7f-OwLCvmWJUQ7Ir_x-ciyd6z_BYGHBA&Issuer=https%3A%2F%2Fapi.vocareum.com&Destination=https%3A%2F%2Fconsole.aws.amazon.com%2Fconsole%2Fhome%3Fregion%3Dus-east-1\" target=\"_blank\">GO TO AWS CONSOLE</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../.aws/aws_console_url', 'r') as file:\n",
    "    aws_url = file.read().strip()\n",
    "\n",
    "HTML(f'<a href=\"{aws_url}\" target=\"_blank\">GO TO AWS CONSOLE</a>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note:* If you see the window like in the following printscreen, click on **logout** link, close the window and click on console link again.\n",
    "\n",
    "![AWSLogout](images/AWSLogout.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2.3. You need the ARN of the roles or users whom you are granting permissions. The required functions are in the Python file `scripts/lf_utils.py`.\n",
    "\n",
    "You will use the `iam_client` and the `lf_utils.get_role_arn` function. You first will grant data location access on the _Data Lake Bucket_ passing the ARN from the bucket to the `lf_utils.grant_data_location_access`, then you will grant access to the databases using `lf_utils.grant_database_access`. The two databases have already been set up in the AWS Glue Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Grant Data location access to: arn:aws:iam::883041721666:role/de-c3w2a1-glue-role \n",
      "\n",
      "INFO:root:Grant database curated_zone access to:arn:aws:iam::883041721666:role/de-c3w2a1-glue-role\n",
      "\n",
      "INFO:root:Grant database presentation_zone access to:arn:aws:iam::883041721666:role/de-c3w2a1-glue-role\n",
      "\n"
     ]
    }
   ],
   "source": [
    "glue_job_arn = lf_utils.get_role_arn(iam_client,'de-c3w2a1-glue-role')\n",
    "lf_utils.grant_data_location_access(lf_client, glue_job_arn, DATA_LOCATION_ARN)\n",
    "lf_utils.grant_database_access(lf_client, glue_job_arn, CURATED_DATABASE_NAME)\n",
    "lf_utils.grant_database_access(lf_client, glue_job_arn, PRESENTATION_DATABASE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2.4. Run the following command in the terminal to set up the environment:\n",
    "\n",
    "```bash\n",
    "source scripts/setup.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "## 2 - Architecture of the Data Lakehouse\n",
    "\n",
    "Here's the proposed architecture of the Data Lakehouse. This architecture can serve as a solution to the problem introduced in this lab. It extracts the data from both source systems and uses AWS Glue jobs to transform and store the data into the different medallion zones, making it queryable for the data analyst through Amazon Athena.\n",
    "\n",
    "![lake_diagram](images/Datalakehouse.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a brief description of the components:\n",
    "\n",
    "- **Data Sources:** You have already interacted with the source database in previous labs, the streaming service brings `ratings` data into an S3 bucket which you will call `source_bucket`. \n",
    "\n",
    "- **Medallion Layers:** As discussed previously, the architecture will be separated into three layers. The Glue jobs will be in charge of loading, transforming and storing the data from the source systems already located in AWS until the curated_zone.\n",
    "\n",
    "  - **Landing layer:** This step involves extracting data from the OLTP database in RDS and from the streaming service's S3 bucket and storing the raw data in the `landing_zone`.\n",
    "    - RDS Landing: An AWS Glue Job is used to connect to the RDS database and retrieve the data in CSV format. \n",
    "    - Streaming Landing: An AWS Glue Job is used to ingest the data from the `source_bucket` in S3, which is stored in JSON format. \n",
    "\n",
    "  - **Curated layer:** After extracting the data, Glue jobs will perform data transformation on top of the raw data. In addition to the transformation, the new curated sources will be stored in the `curated_zone` and added to the Glue Catalog.\n",
    "    -  CSV Transformation: For the RDS tables, they will be enriched with metadata, the schema will be enforced and then the data will be stored in parquet format. \n",
    "    -  JSON Transformation: While the streaming data, as the JSON data containing ratings,  will be joined with the data from the OLTP system to create a table to be used for the ML team. This data will be stored as an Apache Iceberg table. \n",
    "\n",
    "  - **Presentation layer:** [Amazon Athena](https://aws.amazon.com/en/athena/), a serverless, interactive analytics service, is used to query the data stored in the `curated_zone` using the Glue Catalog. It enables SQL-like queries without needing to extract the data from S3 and load it into a traditional database. You will create presentation tables in this layer to solve some of the reporting and analytical needs using SQL queries, storing the data in Apache Iceberg format in the `presentation_zone`.\n",
    "\n",
    "- **End Users:** You will also be in charge of managing the Data Lakehouse in the role of administration, you will have to grant access to the data to one user: an employee from the Machine Learning team (`ML_User`). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "## 3 - Landing Zone \n",
    "\n",
    "To start the Data Lakehouse you need to bring the raw data into the Data Lake. You will use Glue Jobs to do it. The scripts for these jobs are located in `terraform/assets/landing_etl_jobs`. You will use Terraform to deploy them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3-1'></a>\n",
    "### 3.1 - RDS and Streaming Landing\n",
    "\n",
    "3.1.1. Open the script `terraform/assets/landing_etl_jobs/de_c3w2a1_batch_ingress.py`. The code is complete, try to understand it using the comments. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1.2. To deploy the corresponding AWS Glue Job with Terraform, create the Glue Connection to the RDS database. Open the file `terraform/modules/landing_etl/glue.tf` and search for `resource \"aws_glue_connection\" \"rds_connection\"`. In the `connection_properties`, complete the [map object](https://developer.hashicorp.com/terraform/language/expressions/types) with the following key-value pairs:\n",
    "\n",
    "- Set `JDBC_CONNECTION_URL` to  `\"jdbc:mysql://${var.host}:${var.port}/${var.database}\"`\n",
    "- Set `USERNAME` to `var.username`\n",
    "- Set `PASSWORD` to `var.password`\n",
    "\n",
    "Make sure that you save the file after making changes in all of the steps for this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1.3. Open the script `terraform/assets/landing_etl_jobs/de_c3w2a1_json_ingress.py`. The code is complete, go through the lines of the code with the comments to understand it generally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1.4 Copy the glue scripts into the Glue script S3 bucket by running the following cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: terraform/assets/landing_etl_jobs/de_c3w2a1_batch_ingress.py to s3://de-c3w2a1-883041721666-us-east-1-scripts/de_c3w2a1_batch_ingress.py\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp ./terraform/assets/landing_etl_jobs/de_c3w2a1_batch_ingress.py s3://{SCRIPTS_BUCKET_NAME}/de_c3w2a1_batch_ingress.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: terraform/assets/landing_etl_jobs/de_c3w2a1_json_ingress.py to s3://de-c3w2a1-883041721666-us-east-1-scripts/de_c3w2a1_json_ingress.py\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp ./terraform/assets/landing_etl_jobs/de_c3w2a1_json_ingress.py s3://{SCRIPTS_BUCKET_NAME}/de_c3w2a1_json_ingress.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3-2'></a>\n",
    "### 3.2 - Deployment\n",
    "\n",
    "3.2.1. Open the `terraform/main.tf` file and make sure that only the `landing_zone` module is uncommented (lines 1-17). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2.2. Go to the `terraform` folder in the terminal, initialize Terraform and deploy the resources running the following commands:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "cd terraform\n",
    "terraform init\n",
    "terraform plan\n",
    "terraform apply\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note*: You will need to type in `yes` and press `Enter` to perform actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will get the outputs from Terraform, including two job names: `glue_bucket_ingestion_job` and `glue_rds_ingestion_job`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note*: If there are errors in the commands or Terraform configuration files, the terminal may crash. \n",
    "When this happens, you will see the following message:\n",
    "\n",
    "![etl_diagram](images/terminal_crash.png)\n",
    "\n",
    "You can reopen the terminal by pressing <code>Ctrl + \\`</code> (or <code>Cmd + \\`</code>) or by navigating to View > Terminal. \n",
    "In the terminal, go again to the Terraform folder (`cd terraform`) and then try \n",
    "rerunning the required commands. The error should now appear in the terminal.\n",
    "If the terminal continues to crash, run the following command instead:\n",
    "`terraform apply -no-color  2> errors.txt`\n",
    "This will create a text file containing the error message without causing the terminal to crash."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2.3. Use the Glue Job names (`de-c3w2a1-bucket-ingestion-etl-job` and `de-c3w2a1-rds-ingestion-etl-job`) from the Terraform output to run each job:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "aws glue start-job-run --job-name <GLUE-JOB-NAME> | jq -r '.JobRunId'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get `JobRunID` in the output. Check the status of the AWS Glue Jobs exchanging the placeholder `<JOB-RUN-ID>` with the output from the previous step:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "aws glue get-job-run --job-name <GLUE-JOB-NAME> --run-id <JOB-RUN-ID> --output text --query \"JobRun.JobRunState\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When each run job has a `SUCCEEDED` status, you can continue with the rest of the lab. Each of them should take around 2 minutes to complete. You can run them at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note*: If the Glue job fails, you can check its status in the AWS Glue console, where an error message will be displayed. This message can help you debug issues in the Glue scripts. After updating the scripts, be sure to rerun the commands in step 3.1.4 to upload the updated scripts to the scripts bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2.4. You can inspect the result of your Glue Jobs by running the `aws cli` command in the following cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE customers/\n",
      "                           PRE employees/\n",
      "                           PRE offices/\n",
      "                           PRE orderdetails/\n",
      "                           PRE orders/\n",
      "                           PRE payments/\n",
      "                           PRE productlines/\n",
      "                           PRE products/\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls s3://{DATA_LAKE_BUCKET_NAME}/landing_zone/rds/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE ratings/\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls s3://{DATA_LAKE_BUCKET_NAME}/landing_zone/json/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4'></a>\n",
    "## 4 - Curated Zone\n",
    "\n",
    "Once you have checked that the data is available in the `landing_zone`. You can continue with the transformations that will be performed over your data to be stored at the `curated_zone`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4-1'></a>\n",
    "### 4.1 - CSV Transformation\n",
    "\n",
    "4.1.1. Go to the `terraform/assets/transform_etl_jobs` and there you will find the three scripts with which you will work to feed the data into the second layer of your Data Lake. The script `de_c3w2a1_batch_transform.py` will be used to perform the transformation job: add metadata and enforce the schema, stored in CSV format. The file already has the majority of the logic you need, you just need to complete a couple of functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1.2. **Add Metadata**. Metadata refers to data that describes data, it can be related to lineage, source, history or version. Complete the function `add_metadata` to add the required metadata following the instructions in the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1.3. **Enforce schema**. Schema enforcement is essential in data engineering as it ensures data consistency and integrity by enforcing predefined data types, structures and constraints. It helps to prevent errors and facilitates seamless data integration and analysis processes. In this case, the Glue Job contains the schema for each table as a `StructType` object. For the columns that are not of type `StringType`, you want to verify that the corresponding column in the `source_pd` Dataframe has the same type. Complete the function `enforce_schema` following the instructions in the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4-2'></a>\n",
    "### 4.2 - JSON Transformation and Apache Iceberg\n",
    "\n",
    "For the transformation of the data that arrives in JSON format, you will execute two Glue Jobs:\n",
    "\n",
    "- One to join the JSON data with the CSV files that you stored in the landing zone.\n",
    "- A second job will take the JSON data and store it in another file format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the two jobs, the result will be stored in the Apache Iceberg format. The Apache Iceberg format is an open standard designed for handling large analytical datasets efficiently. It's designed to provide fast, consistent, and reliable access to large datasets in cloud storage. Under the hood, Iceberg leverages a combination of file formats, metadata management, and transactional capabilities to provide efficient access to large datasets. It typically stores data using columnar file formats such as Parquet or ORC. These formats are optimized for analytical queries. On the other hand, Iceberg maintains extensive metadata alongside the actual data files. This metadata includes information about the table schema, partitioning strategy, file locations, and transaction state. By storing metadata separately from the data files, Iceberg ensures efficient management and access to table metadata.\n",
    "\n",
    "Some of the main features of the Apache Iceberg format are:\n",
    "- **Schema Flexibility**: Iceberg facilitates schema evolution, enabling the addition or removal of table columns without necessitating a full dataset rewrite. This ensures seamless backward compatibility as data structures evolve over time.\n",
    "- **Transactional Integrity**: Iceberg tables ensure atomic commits, guaranteeing that all table changes are transactional. This ensures that updates are either fully executed or entirely rolled back, maintaining data consistency and reliability.\n",
    "- **Data Partitioning**: Iceberg supports data partitioning based on one or more columns, enhancing query performance by minimizing data scan volumes. Particularly beneficial for large datasets.\n",
    "- **Comprehensive Metadata Management**: Iceberg maintains comprehensive metadata alongside the data, including schema details, partitioning specifications, and file locations. This metadata is stored separately from the data files, simplifying management and queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare the scripts for the Glue Job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2.1. First, you will save the data that is in JSON format into Apache Iceberg. Open the file `terraform/assets/transform_etl_jobs/de_c3w2a1_ratings_to_iceberg.py` and search for the variable named `SqlQuery0`. Complete the provided query with the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "select * \n",
    "from ratings \n",
    "where ingest_ts = (select max(ingest_ts) from ratings)\n",
    ";\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will pull out the ratings with the most recent `ingest_ts` timestamp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2.2. Have a look at the query stored in the variable `SqlQuery1`. You will have to modify it in a later stage in the lab, but in the meantime, try to understand the purpose of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2.3. Open the file located at `terraform/modules/transform_etl/glue.tf`. Search for the `resource \"aws_glue_job\" \"ratings_to_iceberg_job\"` resource. Complete the following parameters:\n",
    "\n",
    "- Set the `timeout` parameter to 7.\n",
    "- Set the number of workers to 2.\n",
    "- In `default_arguments`, set the `--job-language` argument to `\"python\"`. Create another parameter named `--datalake-formats` and set the value to `\"iceberg\"` in the last line of the default parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2.4. Open the glue script `terraform/assets/transform_etl_jobs/de_c3w2a1_json_transform.py`. This script is in charge of unifying some data that you extracted from the RDS Database with the ratings in JSON format so the ML Team can use it to create some models. Search for the variable named `SqlQuery1` and complete the query following the instructions in the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2.5 Copy the glue scripts into the Glue script S3 bucket by run the following cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: terraform/assets/transform_etl_jobs/de_c3w2a1_json_transform.py to s3://de-c3w2a1-883041721666-us-east-1-scripts/de_c3w2a1_json_transform.py\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp ./terraform/assets/transform_etl_jobs/de_c3w2a1_json_transform.py s3://{SCRIPTS_BUCKET_NAME}/de_c3w2a1_json_transform.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: terraform/assets/transform_etl_jobs/de_c3w2a1_batch_transform.py to s3://de-c3w2a1-883041721666-us-east-1-scripts/de_c3w2a1_batch_transform.py\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp ./terraform/assets/transform_etl_jobs/de_c3w2a1_batch_transform.py s3://{SCRIPTS_BUCKET_NAME}/de_c3w2a1_batch_transform.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: terraform/assets/transform_etl_jobs/de_c3w2a1_ratings_to_iceberg.py to s3://de-c3w2a1-883041721666-us-east-1-scripts/de_c3w2a1_ratings_to_iceberg.py\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp ./terraform/assets/transform_etl_jobs/de_c3w2a1_ratings_to_iceberg.py s3://{SCRIPTS_BUCKET_NAME}/de_c3w2a1_ratings_to_iceberg.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4-3'></a>\n",
    "### 4.3 - Deployment\n",
    "\n",
    "4.3.1. Open the file `terraform/main.tf` and uncomment the module named `transform_etl` (lines 19-33)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.3.2. Open the file `terraform/outputs.tf` and uncomment the outputs associated with the `transform_etl` module (lines 15-25)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.3.3. Deploy the Glue Jobs following the same steps as in section [3.2](#3.2). You will need to execute three Glue Jobs: `glue_csv_transform_job`, `glue_ratings_transform_job`, `glue_ratings_to_iceberg_job` (job names `de-c3w2a1-csv-transformation-job`, `de-c3w2a1-ratings-transformation-job` and `de-c3w2a1-ratings-to-iceberg-job`). Each of them should take around 3 minutes, you can run them at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note*: If the Glue job fails, you can check its status in the AWS Glue console, where an error message will be displayed. This message can help you debug issues in the Glue scripts. After updating the scripts, be sure to rerun the commands in step 4.2.5 to upload the updated scripts to the scripts bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.3.4. When each run job has a `SUCCEEDED` status, you can continue with the rest of the lab. Use the `aws s3 ls` command to inspect again the result of your transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE iceberg/\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls s3://{DATA_LAKE_BUCKET_NAME}/curated_zone/ratings/iceberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE customers/\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls s3://{DATA_LAKE_BUCKET_NAME}/curated_zone/customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5'></a>\n",
    "## 5 - Presentation zone\n",
    "\n",
    "For the presentation zone, you will use AWS Athena. You will be using the `awswrangler` library to run `CREATE TABLE AS` queries on the `presentation_zone`, this is an example of how it works. \n",
    "\n",
    "*Note*: `awswrangler` uses the `pyarrow` library that has some functions returning a `FutureWarning` due to deprecation, this doesn't affect what you are trying to do so you can filter out those warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore', FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.1. First, grant access to the tables in the `curated_zone` to the voclabs role (the one used for the lab). You will use a Glue client with `boto3` to obtain the table names in the catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Grant table customers access to:arn:aws:iam::883041721666:role/voclabs\n",
      "\n",
      "INFO:root:Grant table employees access to:arn:aws:iam::883041721666:role/voclabs\n",
      "\n",
      "INFO:root:Grant table offices access to:arn:aws:iam::883041721666:role/voclabs\n",
      "\n",
      "INFO:root:Grant table orderdetails access to:arn:aws:iam::883041721666:role/voclabs\n",
      "\n",
      "INFO:root:Grant table orders access to:arn:aws:iam::883041721666:role/voclabs\n",
      "\n",
      "INFO:root:Grant table payments access to:arn:aws:iam::883041721666:role/voclabs\n",
      "\n",
      "INFO:root:Grant table productlines access to:arn:aws:iam::883041721666:role/voclabs\n",
      "\n",
      "INFO:root:Grant table products access to:arn:aws:iam::883041721666:role/voclabs\n",
      "\n",
      "INFO:root:Grant table ratings access to:arn:aws:iam::883041721666:role/voclabs\n",
      "\n",
      "INFO:root:Grant table ratings_for_ml access to:arn:aws:iam::883041721666:role/voclabs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "curated_tables = glue_client.get_tables(DatabaseName=CURATED_DATABASE_NAME)\n",
    "curated_tables_list = curated_tables[\"TableList\"]\n",
    "curated_table_names = [tableDict[\"Name\"] for tableDict in curated_tables_list]\n",
    "for table in curated_table_names:\n",
    "    lf_utils.grant_table_access(lf_client, VOCLABS_ARN, CURATED_DATABASE_NAME, table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.2. This is an example of creating an Iceberg Table based on a query using Athena and the `curated_zone` tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCEEDED\n"
     ]
    }
   ],
   "source": [
    "# 1. Apagar a tabela do cat치logo\n",
    "wr.catalog.delete_table_if_exists(\n",
    "    database=PRESENTATION_DATABASE_NAME,\n",
    "    table='ratings'\n",
    ")\n",
    "\n",
    "# 2. Apagar os dados no S3\n",
    "wr.s3.delete_objects(\n",
    "    path=f's3://{DATA_LAKE_BUCKET_NAME}/presentation_zone/ratings/'\n",
    ")\n",
    "\n",
    "# 3. Agora executar o CTAS novamente\n",
    "ctas_query = f\"\"\"CREATE TABLE ratings WITH (\n",
    "\ttable_type = 'ICEBERG',\n",
    "\tlocation = 's3://{DATA_LAKE_BUCKET_NAME}/presentation_zone/ratings',\n",
    "\tis_external = false\n",
    ") AS\n",
    "SELECT *\n",
    "FROM {CURATED_DATABASE_NAME}.ratings;\"\"\"\n",
    "response = wr.athena.start_query_execution(\n",
    "    sql=ctas_query,\n",
    "    database=PRESENTATION_DATABASE_NAME,\n",
    "    wait=True,\n",
    "    s3_output = f's3://{DATA_LAKE_BUCKET_NAME}/athena_output/ratings'\n",
    ")\n",
    "print(response['Status']['State'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.3. In the previous example, you made the `ratings` table available in the `presentation` layer for data analysts to use. Bring the table `ratings_for_ml` from the `curated zone` into the `presentation` layer, cast the `process_ts` column to `varchar`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCEEDED\n"
     ]
    }
   ],
   "source": [
    "# 1. Apagar a tabela do cat치logo\n",
    "wr.catalog.delete_table_if_exists(\n",
    "    database=PRESENTATION_DATABASE_NAME,\n",
    "    table='ratings_for_ml'\n",
    ")\n",
    "\n",
    "# 2. Apagar os dados no S3\n",
    "wr.s3.delete_objects(\n",
    "    path=f's3://{DATA_LAKE_BUCKET_NAME}/presentation_zone/ratings_for_ml/'\n",
    ")\n",
    "\n",
    "# 3. Agora executar o CTAS\n",
    "ctas_query = f\"\"\"CREATE TABLE ratings_for_ml WITH (\n",
    "\ttable_type = 'ICEBERG',\n",
    "\tlocation = 's3://{DATA_LAKE_BUCKET_NAME}/presentation_zone/ratings_for_ml',\n",
    "\tis_external = false\n",
    ") AS\n",
    "SELECT customerNumber, city, state, postalCode, country, creditLimit, productCode,productLine, productScale, quantityinstock, buyprice, msrp, productRating, cast(process_ts as varchar) as process_ts\n",
    "FROM {CURATED_DATABASE_NAME}.ratings_for_ml;\"\"\"\n",
    "response = wr.athena.start_query_execution(\n",
    "    sql=ctas_query,\n",
    "    database=PRESENTATION_DATABASE_NAME,\n",
    "    wait=True,\n",
    "    s3_output = f's3://{DATA_LAKE_BUCKET_NAME}/athena_output/ratings_for_ml'\n",
    ")\n",
    "print(response['Status']['State'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.4. Now you will create tables that may involve running aggregations or joins across multiple tables from the curated zone. The first one will involve returning the average sales per month and year and storing them in the table `sales_report`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCEEDED\n"
     ]
    }
   ],
   "source": [
    "# 1. Apagar a tabela do cat치logo\n",
    "wr.catalog.delete_table_if_exists(\n",
    "    database=PRESENTATION_DATABASE_NAME,\n",
    "    table='sales_report'\n",
    ")\n",
    "\n",
    "# 2. Apagar os dados no S3\n",
    "wr.s3.delete_objects(\n",
    "    path=f's3://{DATA_LAKE_BUCKET_NAME}/presentation_zone/sales_report/'\n",
    ")\n",
    "\n",
    "# 3. Agora executar o CTAS\n",
    "ctas_query = f\"\"\"CREATE TABLE sales_report WITH (\n",
    "\ttable_type = 'ICEBERG',\n",
    "\tlocation = 's3://{DATA_LAKE_BUCKET_NAME}/presentation_zone/sales_report',\n",
    "\tis_external = false\n",
    ") AS select year(orderdate) as sales_year, month(orderdate) as sales_month, sum(od.quantityordered * od.priceeach) as sales_total\n",
    "from {CURATED_DATABASE_NAME}.orders o\n",
    "left join {CURATED_DATABASE_NAME}.orderdetails od\n",
    "on o.ordernumber = od.ordernumber\n",
    "group by year(orderdate), month(orderdate)\n",
    "order by year(orderdate), month(orderdate);\"\"\"\n",
    "response = wr.athena.start_query_execution(\n",
    "    sql=ctas_query,\n",
    "    database=PRESENTATION_DATABASE_NAME,\n",
    "    wait=True,\n",
    "    s3_output = f's3://{DATA_LAKE_BUCKET_NAME}/athena_output/sales_report'\n",
    ")\n",
    "print(response['Status']['State'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.5. For the second business object, create a table called `ratings_per_product`, where you calculate the average rating and review count per each product. The table must return the product code, product name, average rating and count of reviews. Order the table by review count in descending order and then by average count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCEEDED\n"
     ]
    }
   ],
   "source": [
    "# 1. Apagar a tabela do cat치logo\n",
    "wr.catalog.delete_table_if_exists(\n",
    "    database=PRESENTATION_DATABASE_NAME,\n",
    "    table='ratings_per_product'\n",
    ")\n",
    "\n",
    "# 2. Apagar os dados no S3\n",
    "wr.s3.delete_objects(\n",
    "    path=f's3://{DATA_LAKE_BUCKET_NAME}/presentation_zone/ratings_per_product/'\n",
    ")\n",
    "\n",
    "# 3. Agora executar o CTAS\n",
    "ctas_query = f\"\"\"\n",
    "CREATE TABLE ratings_per_product WITH (\n",
    "\ttable_type = 'ICEBERG',\n",
    "\tlocation = 's3://{DATA_LAKE_BUCKET_NAME}/presentation_zone/ratings_per_product',\n",
    "\tis_external = false\n",
    ") AS SELECT \t\n",
    "p.productcode,\n",
    "p.productname,\n",
    "avg(productrating) as avg_rating,\n",
    "count(*) as review_count\n",
    "FROM {CURATED_DATABASE_NAME}.products p\n",
    "LEFT JOIN {CURATED_DATABASE_NAME}.ratings r\n",
    "ON p.productcode = r.productcode\n",
    "GROUP BY p.productcode,p.productname\n",
    "ORDER BY review_count DESC,avg_rating;\n",
    "\"\"\"\n",
    "response = wr.athena.start_query_execution(\n",
    "    sql=ctas_query,\n",
    "    database=PRESENTATION_DATABASE_NAME,\n",
    "    wait=True,\n",
    "    s3_output = f's3://{DATA_LAKE_BUCKET_NAME}/athena_output/ratings_per_product'\n",
    ")\n",
    "print(response['Status']['State'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.6. Let's check on the new tables. Grant permissions to the voclabs role on all the tables in the `presentation_zone` database and then query each table, bringing 10 records for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Grant table avg_ratings_per_location access to:arn:aws:iam::883041721666:role/voclabs\n",
      "\n",
      "INFO:root:Grant table ratings access to:arn:aws:iam::883041721666:role/voclabs\n",
      "\n",
      "INFO:root:Grant table ratings_for_ml access to:arn:aws:iam::883041721666:role/voclabs\n",
      "\n",
      "INFO:root:Grant table ratings_per_product access to:arn:aws:iam::883041721666:role/voclabs\n",
      "\n",
      "INFO:root:Grant table sales_report access to:arn:aws:iam::883041721666:role/voclabs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "presentation_tables = glue_client.get_tables(DatabaseName=PRESENTATION_DATABASE_NAME)\n",
    "presentation_tables_list = presentation_tables[\"TableList\"]\n",
    "presentation_table_names = [tableDict[\"Name\"] for tableDict in presentation_tables_list]\n",
    "for table in presentation_table_names:\n",
    "    lf_utils.grant_table_access(lf_client, VOCLABS_ARN, PRESENTATION_DATABASE_NAME, table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:awswrangler.athena._utils:Created CTAS table \"presentation_zone\".\"temp_table_279ce3c589a046beb618819cfae70693\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customernumber</th>\n",
       "      <th>productcode</th>\n",
       "      <th>productrating</th>\n",
       "      <th>ingest_ts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103</td>\n",
       "      <td>S10_2016</td>\n",
       "      <td>3</td>\n",
       "      <td>2025-11-06T12:08:43.690Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103</td>\n",
       "      <td>S24_2022</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-11-06T12:08:43.690Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>112</td>\n",
       "      <td>S24_1046</td>\n",
       "      <td>4</td>\n",
       "      <td>2025-11-06T12:08:43.690Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>112</td>\n",
       "      <td>S18_1342</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-11-06T12:08:43.690Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>114</td>\n",
       "      <td>S10_2016</td>\n",
       "      <td>4</td>\n",
       "      <td>2025-11-06T12:08:43.690Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customernumber productcode  productrating                 ingest_ts\n",
       "0             103    S10_2016              3  2025-11-06T12:08:43.690Z\n",
       "1             103    S24_2022              5  2025-11-06T12:08:43.690Z\n",
       "2             112    S24_1046              4  2025-11-06T12:08:43.690Z\n",
       "3             112    S18_1342              5  2025-11-06T12:08:43.690Z\n",
       "4             114    S10_2016              4  2025-11-06T12:08:43.690Z"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql = 'SELECT * FROM ratings LIMIT 10;'\n",
    "df = wr.athena.read_sql_query(\n",
    "    sql, \n",
    "    database=PRESENTATION_DATABASE_NAME, \n",
    "    s3_output = f's3://{DATA_LAKE_BUCKET_NAME}/athena_output/sample_ratings'\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:awswrangler.athena._utils:Created CTAS table \"presentation_zone\".\"temp_table_0cc68ef3ccb04b2695faca7426df1ab1\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customernumber</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>postalcode</th>\n",
       "      <th>country</th>\n",
       "      <th>creditlimit</th>\n",
       "      <th>productcode</th>\n",
       "      <th>productline</th>\n",
       "      <th>productscale</th>\n",
       "      <th>quantityinstock</th>\n",
       "      <th>buyprice</th>\n",
       "      <th>msrp</th>\n",
       "      <th>productrating</th>\n",
       "      <th>process_ts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>114</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>3004</td>\n",
       "      <td>Australia</td>\n",
       "      <td>117300</td>\n",
       "      <td>S12_1666</td>\n",
       "      <td>Trucks and Buses</td>\n",
       "      <td>1:12</td>\n",
       "      <td>1579</td>\n",
       "      <td>78</td>\n",
       "      <td>137</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-11-06 12:27:06.564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>114</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>3004</td>\n",
       "      <td>Australia</td>\n",
       "      <td>117300</td>\n",
       "      <td>S18_1889</td>\n",
       "      <td>Classic Cars</td>\n",
       "      <td>1:18</td>\n",
       "      <td>8826</td>\n",
       "      <td>54</td>\n",
       "      <td>77</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-11-06 12:27:06.564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>114</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>3004</td>\n",
       "      <td>Australia</td>\n",
       "      <td>117300</td>\n",
       "      <td>S18_2625</td>\n",
       "      <td>Motorcycles</td>\n",
       "      <td>1:18</td>\n",
       "      <td>4357</td>\n",
       "      <td>24</td>\n",
       "      <td>61</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-11-06 12:27:06.564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>114</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>3004</td>\n",
       "      <td>Australia</td>\n",
       "      <td>117300</td>\n",
       "      <td>S18_3320</td>\n",
       "      <td>Vintage Cars</td>\n",
       "      <td>1:18</td>\n",
       "      <td>7913</td>\n",
       "      <td>58</td>\n",
       "      <td>99</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-11-06 12:27:06.564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>114</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>3004</td>\n",
       "      <td>Australia</td>\n",
       "      <td>117300</td>\n",
       "      <td>S24_1785</td>\n",
       "      <td>Planes</td>\n",
       "      <td>1:24</td>\n",
       "      <td>3627</td>\n",
       "      <td>67</td>\n",
       "      <td>109</td>\n",
       "      <td>4</td>\n",
       "      <td>2025-11-06 12:27:06.564</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customernumber       city     state postalcode    country creditlimit  \\\n",
       "0             114  Melbourne  Victoria       3004  Australia      117300   \n",
       "1             114  Melbourne  Victoria       3004  Australia      117300   \n",
       "2             114  Melbourne  Victoria       3004  Australia      117300   \n",
       "3             114  Melbourne  Victoria       3004  Australia      117300   \n",
       "4             114  Melbourne  Victoria       3004  Australia      117300   \n",
       "\n",
       "  productcode       productline productscale  quantityinstock buyprice msrp  \\\n",
       "0    S12_1666  Trucks and Buses         1:12             1579       78  137   \n",
       "1    S18_1889      Classic Cars         1:18             8826       54   77   \n",
       "2    S18_2625       Motorcycles         1:18             4357       24   61   \n",
       "3    S18_3320      Vintage Cars         1:18             7913       58   99   \n",
       "4    S24_1785            Planes         1:24             3627       67  109   \n",
       "\n",
       "   productrating               process_ts  \n",
       "0              5  2025-11-06 12:27:06.564  \n",
       "1              5  2025-11-06 12:27:06.564  \n",
       "2              2  2025-11-06 12:27:06.564  \n",
       "3              5  2025-11-06 12:27:06.564  \n",
       "4              4  2025-11-06 12:27:06.564  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql = 'SELECT * FROM ratings_for_ml LIMIT 10;'\n",
    "df = wr.athena.read_sql_query(\n",
    "    sql, \n",
    "    database=PRESENTATION_DATABASE_NAME, \n",
    "    s3_output = f's3://{DATA_LAKE_BUCKET_NAME}/athena_output/sample_ratings_for_ml'\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:awswrangler.athena._utils:Created CTAS table \"presentation_zone\".\"temp_table_149d4f649dc34bef821a2d5505cee095\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sales_year</th>\n",
       "      <th>sales_month</th>\n",
       "      <th>sales_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2003</td>\n",
       "      <td>1</td>\n",
       "      <td>1.050203e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2003</td>\n",
       "      <td>2</td>\n",
       "      <td>1.155686e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2003</td>\n",
       "      <td>3</td>\n",
       "      <td>1.444734e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2003</td>\n",
       "      <td>4</td>\n",
       "      <td>1.672517e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2003</td>\n",
       "      <td>5</td>\n",
       "      <td>1.614923e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sales_year  sales_month   sales_total\n",
       "0        2003            1  1.050203e+06\n",
       "1        2003            2  1.155686e+06\n",
       "2        2003            3  1.444734e+06\n",
       "3        2003            4  1.672517e+06\n",
       "4        2003            5  1.614923e+06"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql = 'SELECT * FROM sales_report LIMIT 10;'\n",
    "df = wr.athena.read_sql_query(\n",
    "    sql, \n",
    "    database=PRESENTATION_DATABASE_NAME, \n",
    "    s3_output = f's3://{DATA_LAKE_BUCKET_NAME}/athena_output/sample_sales_report'\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:awswrangler.athena._utils:Created CTAS table \"presentation_zone\".\"temp_table_c0cec16d7e854660a54ffa9962c86c91\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>productcode</th>\n",
       "      <th>productname</th>\n",
       "      <th>avg_rating</th>\n",
       "      <th>review_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S18_3232</td>\n",
       "      <td>1992 Ferrari 360 Spider red</td>\n",
       "      <td>3.115385</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S18_1984</td>\n",
       "      <td>1995 Honda Civic</td>\n",
       "      <td>3.368421</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S10_1678</td>\n",
       "      <td>1969 Harley Davidson Ultimate Chopper</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S18_1129</td>\n",
       "      <td>1993 Mazda RX-7</td>\n",
       "      <td>2.777778</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S24_1578</td>\n",
       "      <td>1997 BMW R 1100 S</td>\n",
       "      <td>2.588235</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  productcode                            productname  avg_rating  review_count\n",
       "0    S18_3232            1992 Ferrari 360 Spider red    3.115385            78\n",
       "1    S18_1984                       1995 Honda Civic    3.368421            57\n",
       "2    S10_1678  1969 Harley Davidson Ultimate Chopper    2.500000            54\n",
       "3    S18_1129                        1993 Mazda RX-7    2.777778            54\n",
       "4    S24_1578                      1997 BMW R 1100 S    2.588235            51"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql = 'SELECT * FROM ratings_per_product LIMIT 10;'\n",
    "df = wr.athena.read_sql_query(\n",
    "    sql, \n",
    "    database=PRESENTATION_DATABASE_NAME, \n",
    "    s3_output = f's3://{DATA_LAKE_BUCKET_NAME}/athena_output/sample_ratings_per_product'\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note*: **The next sections 6 and 7 are both optional and not graded. If you choose to skip them, please submit the lab by clicking on `Submit assignment` (top right corner). Section 8 outlines the clean up process in case you want to rerun some Terraform commands (also not graded).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6'></a>\n",
    "## 6 - [Optional and Not Graded] - Some Features of Iceberg Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6-1'></a>\n",
    "### 6.1 - Schema Evolution\n",
    "\n",
    "Now, you will see some of the features that the Iceberg format has. You will focus mostly on the schema evolution and querying data from different versions. The team that serves the data in the S3 source bucket has told you that they added a new column holding the date at which the customers performed the rating to the product, named `ratingtimestamp`. The data is hosted in the same bucket, but at the S3 key named `ratings_with_timestamp`. This will be the folder where all new data will continue to be stored. Your task is to add the new column to your `ratings` table at the `curated_zone` database and then extract the new data. Let's use the `awswrangler` package again to perform some queries over AWS Athena. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.1.1. Explore the current available tables in your `curated_zone` database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Database</th>\n",
       "      <th>Table</th>\n",
       "      <th>Description</th>\n",
       "      <th>TableType</th>\n",
       "      <th>Columns</th>\n",
       "      <th>Partitions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>curated_zone</td>\n",
       "      <td>customers</td>\n",
       "      <td></td>\n",
       "      <td>EXTERNAL_TABLE</td>\n",
       "      <td>customernumber, customername, contactlastname,...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>curated_zone</td>\n",
       "      <td>employees</td>\n",
       "      <td></td>\n",
       "      <td>EXTERNAL_TABLE</td>\n",
       "      <td>employeenumber, lastname, firstname, extension...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>curated_zone</td>\n",
       "      <td>offices</td>\n",
       "      <td></td>\n",
       "      <td>EXTERNAL_TABLE</td>\n",
       "      <td>officecode, city, phone, addressline1, address...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>curated_zone</td>\n",
       "      <td>orderdetails</td>\n",
       "      <td></td>\n",
       "      <td>EXTERNAL_TABLE</td>\n",
       "      <td>ordernumber, productcode, quantityordered, pri...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>curated_zone</td>\n",
       "      <td>orders</td>\n",
       "      <td></td>\n",
       "      <td>EXTERNAL_TABLE</td>\n",
       "      <td>ordernumber, orderdate, requireddate, shippedd...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>curated_zone</td>\n",
       "      <td>payments</td>\n",
       "      <td></td>\n",
       "      <td>EXTERNAL_TABLE</td>\n",
       "      <td>customernumber, checknumber, paymentdate, amou...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>curated_zone</td>\n",
       "      <td>productlines</td>\n",
       "      <td></td>\n",
       "      <td>EXTERNAL_TABLE</td>\n",
       "      <td>productline, textdescription, htmldescription,...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>curated_zone</td>\n",
       "      <td>products</td>\n",
       "      <td></td>\n",
       "      <td>EXTERNAL_TABLE</td>\n",
       "      <td>productcode, productname, productline, product...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>curated_zone</td>\n",
       "      <td>ratings</td>\n",
       "      <td></td>\n",
       "      <td>EXTERNAL_TABLE</td>\n",
       "      <td>customernumber, productcode, productrating, in...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>curated_zone</td>\n",
       "      <td>ratings_for_ml</td>\n",
       "      <td></td>\n",
       "      <td>EXTERNAL_TABLE</td>\n",
       "      <td>customernumber, city, state, postalcode, count...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Database           Table Description       TableType  \\\n",
       "0  curated_zone       customers              EXTERNAL_TABLE   \n",
       "1  curated_zone       employees              EXTERNAL_TABLE   \n",
       "2  curated_zone         offices              EXTERNAL_TABLE   \n",
       "3  curated_zone    orderdetails              EXTERNAL_TABLE   \n",
       "4  curated_zone          orders              EXTERNAL_TABLE   \n",
       "5  curated_zone        payments              EXTERNAL_TABLE   \n",
       "6  curated_zone    productlines              EXTERNAL_TABLE   \n",
       "7  curated_zone        products              EXTERNAL_TABLE   \n",
       "8  curated_zone         ratings              EXTERNAL_TABLE   \n",
       "9  curated_zone  ratings_for_ml              EXTERNAL_TABLE   \n",
       "\n",
       "                                             Columns Partitions  \n",
       "0  customernumber, customername, contactlastname,...             \n",
       "1  employeenumber, lastname, firstname, extension...             \n",
       "2  officecode, city, phone, addressline1, address...             \n",
       "3  ordernumber, productcode, quantityordered, pri...             \n",
       "4  ordernumber, orderdate, requireddate, shippedd...             \n",
       "5  customernumber, checknumber, paymentdate, amou...             \n",
       "6  productline, textdescription, htmldescription,...             \n",
       "7  productcode, productname, productline, product...             \n",
       "8  customernumber, productcode, productrating, in...             \n",
       "9  customernumber, city, state, postalcode, count...             "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wr.catalog.tables(database=CURATED_DATABASE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.1.2. You can see the `ratings` table there with some of the current columns. Query the table `ratings` at its current state. Create a query to select all the columns from the `ratings` table; limit your search to 10 rows. Save this query as a string into the `sql` variable and then execute the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:awswrangler.athena._utils:Created CTAS table \"curated_zone\".\"temp_table_b750fa98f2d14f2ab3b798001ce04667\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customernumber</th>\n",
       "      <th>productcode</th>\n",
       "      <th>productrating</th>\n",
       "      <th>ingest_ts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103</td>\n",
       "      <td>S10_2016</td>\n",
       "      <td>3</td>\n",
       "      <td>2025-11-06T12:08:43.690Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103</td>\n",
       "      <td>S24_2022</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-11-06T12:08:43.690Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>112</td>\n",
       "      <td>S24_1046</td>\n",
       "      <td>4</td>\n",
       "      <td>2025-11-06T12:08:43.690Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>112</td>\n",
       "      <td>S24_3969</td>\n",
       "      <td>4</td>\n",
       "      <td>2025-11-06T12:08:43.690Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>112</td>\n",
       "      <td>S18_1889</td>\n",
       "      <td>4</td>\n",
       "      <td>2025-11-06T12:08:43.690Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customernumber productcode  productrating                 ingest_ts\n",
       "0             103    S10_2016              3  2025-11-06T12:08:43.690Z\n",
       "1             103    S24_2022              5  2025-11-06T12:08:43.690Z\n",
       "2             112    S24_1046              4  2025-11-06T12:08:43.690Z\n",
       "3             112    S24_3969              4  2025-11-06T12:08:43.690Z\n",
       "4             112    S18_1889              4  2025-11-06T12:08:43.690Z"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql = \"SELECT * FROM ratings LIMIT 10;\"\n",
    "df = wr.athena.read_sql_query(sql, database=CURATED_DATABASE_NAME, s3_output=f's3://{DATA_LAKE_BUCKET_NAME}/athena_output/')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.1.3. Now, let's add the new column to the `iceberg` table. Open the file `terraform/assets/alter_table_job/de_c3w2a1_alter_ratings_table.py` and search for the function named `add_column()`. There you will see an SQL Statement that performs an `ALTER TABLE` to add a new column. The column name and type are passed as parameters to the Glue Job. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.1.4. Open the file `terraform/main.tf` and uncomment the module named `alter_table` (lines 35-50)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.1.5. Open the file `terraform/outputs.tf` and uncomment the outputs associated with the `alter_table` module (lines 28-30)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.1.6 Copy the glue job using the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: terraform/assets/alter_table_job/de_c3w2a1_alter_ratings_table.py to s3://de-c3w2a1-883041721666-us-east-1-scripts/de_c3w2a1_alter_ratings_table.py\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp ./terraform/assets/alter_table_job/de_c3w2a1_alter_ratings_table.py s3://{SCRIPTS_BUCKET_NAME}/de_c3w2a1_alter_ratings_table.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.1.6. Deploy the Glue Job following the same steps as in section 3.2. The Glue Job name will be in the new output `glue_alter_table_job` (job name `de-c3w2a1-alter-table-job`). It will take about 2 minutes to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.1.7. Once your job has finished with a `SUCCEEDED` status, run the following cell to check that the new column has been added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:awswrangler.athena._utils:Created CTAS table \"curated_zone\".\"temp_table_8ac4c55f5d384f8ebaaa054325d76041\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customernumber</th>\n",
       "      <th>productcode</th>\n",
       "      <th>productrating</th>\n",
       "      <th>ingest_ts</th>\n",
       "      <th>ratingtimestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103</td>\n",
       "      <td>S10_2016</td>\n",
       "      <td>3</td>\n",
       "      <td>2025-11-06T12:08:43.690Z</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103</td>\n",
       "      <td>S24_2022</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-11-06T12:08:43.690Z</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>112</td>\n",
       "      <td>S24_1046</td>\n",
       "      <td>4</td>\n",
       "      <td>2025-11-06T12:08:43.690Z</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>112</td>\n",
       "      <td>S18_1342</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-11-06T12:08:43.690Z</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>114</td>\n",
       "      <td>S10_2016</td>\n",
       "      <td>4</td>\n",
       "      <td>2025-11-06T12:08:43.690Z</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customernumber productcode  productrating                 ingest_ts  \\\n",
       "0             103    S10_2016              3  2025-11-06T12:08:43.690Z   \n",
       "1             103    S24_2022              5  2025-11-06T12:08:43.690Z   \n",
       "2             112    S24_1046              4  2025-11-06T12:08:43.690Z   \n",
       "3             112    S18_1342              5  2025-11-06T12:08:43.690Z   \n",
       "4             114    S10_2016              4  2025-11-06T12:08:43.690Z   \n",
       "\n",
       "  ratingtimestamp  \n",
       "0            <NA>  \n",
       "1            <NA>  \n",
       "2            <NA>  \n",
       "3            <NA>  \n",
       "4            <NA>  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql = \"SELECT * FROM ratings LIMIT 10;\"\n",
    "df = wr.athena.read_sql_query(sql, database=CURATED_DATABASE_NAME, s3_output=f's3://{DATA_LAKE_BUCKET_NAME}/athena_output/')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.1.8. The table schema has been changed, you will need to modify your previous Glue Job to take into account such new changes. Open the Glue Job script `terraform/assets/landing_etl_jobs/de_c3w2a1_json_ingress.py`. Search for the creation of the object named `source_ratings_json`. You can see that in the `connection_options` there is a `\"paths\"` parameter. Change the key in the source data lake from `ratings` to `ratings_with_timestamp`, save the file. Run the following cell to copy the new version of the script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: terraform/assets/landing_etl_jobs/de_c3w2a1_json_ingress.py to s3://de-c3w2a1-883041721666-us-east-1-scripts/de_c3w2a1_json_ingress.py\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp ./terraform/assets/landing_etl_jobs/de_c3w2a1_json_ingress.py s3://{SCRIPTS_BUCKET_NAME}/de_c3w2a1_json_ingress.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.1.9. Run again the corresponding Glue Job associated with the terraform output `glue_bucket_ingestion_job`. Wait until the job has finished with the `SUCCEEDED` status (it will take about 2 minutes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.1.10. Now that the new ratings with the new column have arrived in your `landing_zone`, it is time to use them in your next jobs. For the sake of simplicity, you will only run the job that transforms the data from JSON format into `iceberg`. Open the `terraform/assets/transform_etl_jobs/de_c3w2a1_ratings_to_iceberg.py` file and search for the query that performs a merge into your database. In the `UPDATE SET` statement, add the `ratingtimestamp` column using the same syntax that you can see for the other two columns. Run the following cell to update the script file in the bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: terraform/assets/transform_etl_jobs/de_c3w2a1_ratings_to_iceberg.py to s3://de-c3w2a1-883041721666-us-east-1-scripts/de_c3w2a1_ratings_to_iceberg.py\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp ./terraform/assets/transform_etl_jobs/de_c3w2a1_ratings_to_iceberg.py s3://{SCRIPTS_BUCKET_NAME}/de_c3w2a1_ratings_to_iceberg.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.1.11. Execute again the `glue_ratings_to_iceberg_job`. It will take about 3 minutes to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.1.12. Once the job has finished successfully, execute again a query over the `ratings` table to see the new results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:awswrangler.athena._utils:Created CTAS table \"curated_zone\".\"temp_table_2587ac88976541008bb39a70fc495888\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customernumber</th>\n",
       "      <th>productcode</th>\n",
       "      <th>productrating</th>\n",
       "      <th>ingest_ts</th>\n",
       "      <th>ratingtimestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [customernumber, productcode, productrating, ingest_ts, ratingtimestamp]\n",
       "Index: []"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql = \"SELECT * FROM ratings WHERE ratingtimestamp IS NOT NULL LIMIT 10;\"\n",
    "df = wr.athena.read_sql_query(sql, database=CURATED_DATABASE_NAME, s3_output=f's3://{DATA_LAKE_BUCKET_NAME}/athena_output/')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have realized that the schema of your table has evolved and you were able to add data with a new schema you will explore another feature of the `iceberg` format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6-2'></a>\n",
    "### 6.2 - Versioning with Iceberg\n",
    "\n",
    "One of the advantages of the `iceberg` format is that it allows you to save different versions of your tables. Those versions are stored as snapshots. To check the number of versions/snapshots of your `ratings` table, which is the one that has changed, you can execute the following query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:awswrangler.athena._utils:Created CTAS table \"curated_zone\".\"temp_table_19946fc5449b42c8997964a184f75dcf\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>committed_at</th>\n",
       "      <th>snapshot_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>operation</th>\n",
       "      <th>manifest_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-11-06 12:27:04.729 UTC</td>\n",
       "      <td>4187971742967189123</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>overwrite</td>\n",
       "      <td>s3://de-c3w2a1-883041721666-us-east-1-data-lak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-11-06 12:29:01.654 UTC</td>\n",
       "      <td>9031887904386506862</td>\n",
       "      <td>4187971742967189123</td>\n",
       "      <td>overwrite</td>\n",
       "      <td>s3://de-c3w2a1-883041721666-us-east-1-data-lak...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  committed_at          snapshot_id            parent_id  \\\n",
       "0  2025-11-06 12:27:04.729 UTC  4187971742967189123                 <NA>   \n",
       "1  2025-11-06 12:29:01.654 UTC  9031887904386506862  4187971742967189123   \n",
       "\n",
       "   operation                                      manifest_list  \n",
       "0  overwrite  s3://de-c3w2a1-883041721666-us-east-1-data-lak...  \n",
       "1  overwrite  s3://de-c3w2a1-883041721666-us-east-1-data-lak...  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql = 'SELECT CAST(committed_at AS VARCHAR) as committed_at, snapshot_id,  parent_id, operation, manifest_list FROM \"ratings$snapshots\" order by committed_at asc;'\n",
    "df = wr.athena.read_sql_query(sql, database=CURATED_DATABASE_NAME, s3_output=f's3://{DATA_LAKE_BUCKET_NAME}/athena_output/')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see snapshot ID, the time at which that snapshot was created (`committed_at`) and also a `parent_id`, which corresponds to the previous version of the `snapshot_id`. To check the data that was stored at a particular version of your table, you can add the `FOR VERSION AS OF` statement at the end of your query, followed by the snapshot ID that you want to query. Complete the following query with the snapshot ID of your first table version (this is the one that has no `parent_id`) replacing the placeholder `<FIRST-SNAPSHOT-ID>`. Then complete another cell with the last snapshot ID (replacing `<LAST-SNAPSHOT-ID>`); to request the data of the last version of your data. You can execute the two of them to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:awswrangler.athena._utils:Created CTAS table \"curated_zone\".\"temp_table_5e03f3f377bc4c6f966df98f695fba59\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customernumber</th>\n",
       "      <th>productcode</th>\n",
       "      <th>productrating</th>\n",
       "      <th>ingest_ts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103</td>\n",
       "      <td>S10_2016</td>\n",
       "      <td>3</td>\n",
       "      <td>2025-11-06T12:08:43.690Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103</td>\n",
       "      <td>S24_2022</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-11-06T12:08:43.690Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>112</td>\n",
       "      <td>S24_1046</td>\n",
       "      <td>4</td>\n",
       "      <td>2025-11-06T12:08:43.690Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>112</td>\n",
       "      <td>S24_3969</td>\n",
       "      <td>4</td>\n",
       "      <td>2025-11-06T12:08:43.690Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>112</td>\n",
       "      <td>S18_1889</td>\n",
       "      <td>4</td>\n",
       "      <td>2025-11-06T12:08:43.690Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customernumber productcode  productrating                 ingest_ts\n",
       "0             103    S10_2016              3  2025-11-06T12:08:43.690Z\n",
       "1             103    S24_2022              5  2025-11-06T12:08:43.690Z\n",
       "2             112    S24_1046              4  2025-11-06T12:08:43.690Z\n",
       "3             112    S24_3969              4  2025-11-06T12:08:43.690Z\n",
       "4             112    S18_1889              4  2025-11-06T12:08:43.690Z"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql = 'SELECT * FROM ratings FOR VERSION AS OF 4187971742967189123;'\n",
    "df = wr.athena.read_sql_query(sql, database=CURATED_DATABASE_NAME, s3_output=f's3://{DATA_LAKE_BUCKET_NAME}/athena_output/')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:awswrangler.athena._utils:Created CTAS table \"curated_zone\".\"temp_table_9e162a19d3e446228811bd2112a0ba38\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customernumber</th>\n",
       "      <th>productcode</th>\n",
       "      <th>productrating</th>\n",
       "      <th>ingest_ts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103</td>\n",
       "      <td>S10_2016</td>\n",
       "      <td>3</td>\n",
       "      <td>2025-11-06T12:08:43.690Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103</td>\n",
       "      <td>S24_2022</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-11-06T12:08:43.690Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>112</td>\n",
       "      <td>S24_1046</td>\n",
       "      <td>4</td>\n",
       "      <td>2025-11-06T12:08:43.690Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>112</td>\n",
       "      <td>S24_3969</td>\n",
       "      <td>4</td>\n",
       "      <td>2025-11-06T12:08:43.690Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>112</td>\n",
       "      <td>S18_1889</td>\n",
       "      <td>4</td>\n",
       "      <td>2025-11-06T12:08:43.690Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customernumber productcode  productrating                 ingest_ts\n",
       "0             103    S10_2016              3  2025-11-06T12:08:43.690Z\n",
       "1             103    S24_2022              5  2025-11-06T12:08:43.690Z\n",
       "2             112    S24_1046              4  2025-11-06T12:08:43.690Z\n",
       "3             112    S24_3969              4  2025-11-06T12:08:43.690Z\n",
       "4             112    S18_1889              4  2025-11-06T12:08:43.690Z"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql = 'SELECT * FROM ratings FOR VERSION AS OF 4187971742967189123;'\n",
    "df = wr.athena.read_sql_query(sql, database=CURATED_DATABASE_NAME, s3_output=f's3://{DATA_LAKE_BUCKET_NAME}/athena_output/')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this exercise, you have used the data versioning feature of Apache Iceberg which allows users to track changes to datasets over time by creating immutable snapshots of the data at different points in time. Two important points to take into account are:\n",
    "\n",
    "- When a user creates a new version of a dataset in Iceberg, Iceberg creates an immutable snapshot of the dataset's state at that point in time. This snapshot includes all the data files and metadata associated with the dataset.\n",
    "- On the other hand, Iceberg maintains metadata to track the history of dataset versions. Each version is assigned a unique identifier, typically a timestamp or version number, and includes information about the changes made in that version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='7'></a>\n",
    "## 7 - [Optional and Not Graded] - Granting Permissions with Lake Formation\n",
    "\n",
    "In your company there is a Machine Learning team that will use the results of your pipelines, in particular, they will use the `ratings_for_ml` table that is stored in the `presentation_zone` database. They use the AWS user `ml_data_lake_user` to interact with AWS resources and query the data from the `ratings_for_ml` table. Your task is to provide them only with the necessary access to the information, which means, they will only be able to query the `ratings_for_ml` table and shouldn't have access to any other table in the `presentation_zone` database. In the same way, they shouldn't be able to modify the data in that table.\n",
    "\n",
    "You have access to the credentials of `ml_data_lake_user` to test the permissions that you will grant in this section. Run the following cell to grab the credentials from the AWS Secret Manager service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "secrets_client = boto3.client('secretsmanager')\n",
    "response = secrets_client.get_secret_value(SecretId='/datalake/credentials/ml_data_lake_user')\n",
    "crendentials = json.loads(response['SecretString'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you will create a `boto3` session with those credentials to access AWS resources as `ml_data_lake_user`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.Session(aws_access_key_id = crendentials['ACCESS_KEY'], aws_secret_access_key = crendentials['SECRET_KEY'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass the `boto3` session you just created to the `read_sql_query` method below through the `boto3_session` parameter and execute the query. The `ml_data_lake_user` user doesn't have permission over the Data Lake Bucket so you will use an AWS Athena workgroup to execute the query. This query is expected to fail due to insufficient Lakeformation Permissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "AccessDeniedException",
     "evalue": "An error occurred (AccessDeniedException) when calling the DeleteTable operation: Insufficient Lake Formation permission(s): Required Drop on temp_table_d847352caa504a3cbaed6b5b2cbeed6c",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mQueryFailed\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda/lib/python3.11/site-packages/awswrangler/athena/_read.py:506\u001b[0m, in \u001b[0;36m_resolve_query_without_cache\u001b[0;34m(sql, database, data_source, ctas_approach, unload_approach, unload_parameters, categories, chunksize, s3_output, workgroup, encryption, kms_key, keep_files, ctas_database, ctas_temp_table_name, ctas_bucketing_info, ctas_write_compression, athena_query_wait_polling_delay, use_threads, s3_additional_kwargs, boto3_session, pyarrow_additional_kwargs, execution_params, dtype_backend, client_request_token)\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 506\u001b[0m     \u001b[39mreturn\u001b[39;00m _resolve_query_without_cache_ctas(\n\u001b[1;32m    507\u001b[0m         sql\u001b[39m=\u001b[39msql,\n\u001b[1;32m    508\u001b[0m         database\u001b[39m=\u001b[39mdatabase,\n\u001b[1;32m    509\u001b[0m         data_source\u001b[39m=\u001b[39mdata_source,\n\u001b[1;32m    510\u001b[0m         s3_output\u001b[39m=\u001b[39ms3_output,\n\u001b[1;32m    511\u001b[0m         keep_files\u001b[39m=\u001b[39mkeep_files,\n\u001b[1;32m    512\u001b[0m         chunksize\u001b[39m=\u001b[39mchunksize,\n\u001b[1;32m    513\u001b[0m         categories\u001b[39m=\u001b[39mcategories,\n\u001b[1;32m    514\u001b[0m         encryption\u001b[39m=\u001b[39mencryption,\n\u001b[1;32m    515\u001b[0m         workgroup\u001b[39m=\u001b[39mworkgroup,\n\u001b[1;32m    516\u001b[0m         kms_key\u001b[39m=\u001b[39mkms_key,\n\u001b[1;32m    517\u001b[0m         alt_database\u001b[39m=\u001b[39mctas_database,\n\u001b[1;32m    518\u001b[0m         name\u001b[39m=\u001b[39mname,\n\u001b[1;32m    519\u001b[0m         ctas_bucketing_info\u001b[39m=\u001b[39mctas_bucketing_info,\n\u001b[1;32m    520\u001b[0m         ctas_write_compression\u001b[39m=\u001b[39mctas_write_compression,\n\u001b[1;32m    521\u001b[0m         athena_query_wait_polling_delay\u001b[39m=\u001b[39mathena_query_wait_polling_delay,\n\u001b[1;32m    522\u001b[0m         use_threads\u001b[39m=\u001b[39muse_threads,\n\u001b[1;32m    523\u001b[0m         s3_additional_kwargs\u001b[39m=\u001b[39ms3_additional_kwargs,\n\u001b[1;32m    524\u001b[0m         boto3_session\u001b[39m=\u001b[39mboto3_session,\n\u001b[1;32m    525\u001b[0m         pyarrow_additional_kwargs\u001b[39m=\u001b[39mpyarrow_additional_kwargs,\n\u001b[1;32m    526\u001b[0m         execution_params\u001b[39m=\u001b[39mexecution_params,\n\u001b[1;32m    527\u001b[0m         dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[1;32m    528\u001b[0m     )\n\u001b[1;32m    529\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda/lib/python3.11/site-packages/awswrangler/athena/_read.py:324\u001b[0m, in \u001b[0;36m_resolve_query_without_cache_ctas\u001b[0;34m(sql, database, data_source, s3_output, keep_files, chunksize, categories, encryption, workgroup, kms_key, alt_database, name, ctas_bucketing_info, ctas_write_compression, athena_query_wait_polling_delay, use_threads, s3_additional_kwargs, boto3_session, pyarrow_additional_kwargs, execution_params, dtype_backend)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_resolve_query_without_cache_ctas\u001b[39m(\n\u001b[1;32m    302\u001b[0m     sql: \u001b[39mstr\u001b[39m,\n\u001b[1;32m    303\u001b[0m     database: \u001b[39mstr\u001b[39m \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    322\u001b[0m     dtype_backend: Literal[\u001b[39m\"\u001b[39m\u001b[39mnumpy_nullable\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mpyarrow\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnumpy_nullable\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    323\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m pd\u001b[39m.\u001b[39mDataFrame \u001b[39m|\u001b[39m Iterator[pd\u001b[39m.\u001b[39mDataFrame]:\n\u001b[0;32m--> 324\u001b[0m     ctas_query_info: \u001b[39mdict\u001b[39m[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m \u001b[39m|\u001b[39m _QueryMetadata] \u001b[39m=\u001b[39m create_ctas_table(\n\u001b[1;32m    325\u001b[0m         sql\u001b[39m=\u001b[39msql,\n\u001b[1;32m    326\u001b[0m         database\u001b[39m=\u001b[39mdatabase,\n\u001b[1;32m    327\u001b[0m         ctas_table\u001b[39m=\u001b[39mname,\n\u001b[1;32m    328\u001b[0m         ctas_database\u001b[39m=\u001b[39malt_database,\n\u001b[1;32m    329\u001b[0m         bucketing_info\u001b[39m=\u001b[39mctas_bucketing_info,\n\u001b[1;32m    330\u001b[0m         data_source\u001b[39m=\u001b[39mdata_source,\n\u001b[1;32m    331\u001b[0m         s3_output\u001b[39m=\u001b[39ms3_output,\n\u001b[1;32m    332\u001b[0m         workgroup\u001b[39m=\u001b[39mworkgroup,\n\u001b[1;32m    333\u001b[0m         encryption\u001b[39m=\u001b[39mencryption,\n\u001b[1;32m    334\u001b[0m         write_compression\u001b[39m=\u001b[39mctas_write_compression,\n\u001b[1;32m    335\u001b[0m         kms_key\u001b[39m=\u001b[39mkms_key,\n\u001b[1;32m    336\u001b[0m         wait\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    337\u001b[0m         athena_query_wait_polling_delay\u001b[39m=\u001b[39mathena_query_wait_polling_delay,\n\u001b[1;32m    338\u001b[0m         boto3_session\u001b[39m=\u001b[39mboto3_session,\n\u001b[1;32m    339\u001b[0m         execution_params\u001b[39m=\u001b[39mexecution_params,\n\u001b[1;32m    340\u001b[0m     )\n\u001b[1;32m    341\u001b[0m     fully_qualified_name: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mctas_query_info[\u001b[39m\"\u001b[39m\u001b[39mctas_database\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mctas_query_info[\u001b[39m\"\u001b[39m\u001b[39mctas_table\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\n",
      "File \u001b[0;32m~/miniconda/lib/python3.11/site-packages/awswrangler/_config.py:735\u001b[0m, in \u001b[0;36mapply_configs.<locals>.wrapper\u001b[0;34m(*args_raw, **kwargs)\u001b[0m\n\u001b[1;32m    734\u001b[0m         args \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkeywords}\n\u001b[0;32m--> 735\u001b[0m \u001b[39mreturn\u001b[39;00m function(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39margs)\n",
      "File \u001b[0;32m~/miniconda/lib/python3.11/site-packages/awswrangler/athena/_utils.py:844\u001b[0m, in \u001b[0;36mcreate_ctas_table\u001b[0;34m(sql, database, ctas_table, ctas_database, s3_output, storage_format, write_compression, partitioning_info, bucketing_info, field_delimiter, schema_only, workgroup, data_source, encryption, kms_key, categories, wait, athena_query_wait_polling_delay, execution_params, boto3_session)\u001b[0m\n\u001b[1;32m    840\u001b[0m             \u001b[39mraise\u001b[39;00m exceptions\u001b[39m.\u001b[39mInvalidArgumentValue(\n\u001b[1;32m    841\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mPlease, don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt leave undefined columns types in your query. You can cast to ensure it. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    842\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m(E.g. \u001b[39m\u001b[39m'\u001b[39m\u001b[39mSELECT CAST(NULL AS INTEGER) AS MY_COL, ...\u001b[39m\u001b[39m'\u001b[39m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    843\u001b[0m             )\n\u001b[0;32m--> 844\u001b[0m         \u001b[39mraise\u001b[39;00m ex\n\u001b[1;32m    845\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda/lib/python3.11/site-packages/awswrangler/athena/_utils.py:822\u001b[0m, in \u001b[0;36mcreate_ctas_table\u001b[0;34m(sql, database, ctas_table, ctas_database, s3_output, storage_format, write_compression, partitioning_info, bucketing_info, field_delimiter, schema_only, workgroup, data_source, encryption, kms_key, categories, wait, athena_query_wait_polling_delay, execution_params, boto3_session)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 822\u001b[0m     response[\u001b[39m\"\u001b[39m\u001b[39mctas_query_metadata\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m _get_query_metadata(\n\u001b[1;32m    823\u001b[0m         query_execution_id\u001b[39m=\u001b[39mquery_execution_id,\n\u001b[1;32m    824\u001b[0m         boto3_session\u001b[39m=\u001b[39mboto3_session,\n\u001b[1;32m    825\u001b[0m         categories\u001b[39m=\u001b[39mcategories,\n\u001b[1;32m    826\u001b[0m         metadata_cache_manager\u001b[39m=\u001b[39m_cache_manager,\n\u001b[1;32m    827\u001b[0m         athena_query_wait_polling_delay\u001b[39m=\u001b[39mathena_query_wait_polling_delay,\n\u001b[1;32m    828\u001b[0m     )\n\u001b[1;32m    829\u001b[0m \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mQueryFailed \u001b[39mas\u001b[39;00m ex:\n",
      "File \u001b[0;32m~/miniconda/lib/python3.11/site-packages/awswrangler/athena/_utils.py:231\u001b[0m, in \u001b[0;36m_get_query_metadata\u001b[0;34m(query_execution_id, boto3_session, categories, query_execution_payload, metadata_cache_manager, athena_query_wait_polling_delay, execution_params, dtype_backend)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 231\u001b[0m     _query_execution_payload \u001b[39m=\u001b[39m _executions\u001b[39m.\u001b[39mwait_query(\n\u001b[1;32m    232\u001b[0m         query_execution_id\u001b[39m=\u001b[39mquery_execution_id,\n\u001b[1;32m    233\u001b[0m         boto3_session\u001b[39m=\u001b[39mboto3_session,\n\u001b[1;32m    234\u001b[0m         athena_query_wait_polling_delay\u001b[39m=\u001b[39mathena_query_wait_polling_delay,\n\u001b[1;32m    235\u001b[0m     )\n\u001b[1;32m    236\u001b[0m cols_types: \u001b[39mdict\u001b[39m[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m get_query_columns_types(\n\u001b[1;32m    237\u001b[0m     query_execution_id\u001b[39m=\u001b[39mquery_execution_id, boto3_session\u001b[39m=\u001b[39mboto3_session\n\u001b[1;32m    238\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda/lib/python3.11/site-packages/awswrangler/_config.py:735\u001b[0m, in \u001b[0;36mapply_configs.<locals>.wrapper\u001b[0;34m(*args_raw, **kwargs)\u001b[0m\n\u001b[1;32m    734\u001b[0m         args \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkeywords}\n\u001b[0;32m--> 735\u001b[0m \u001b[39mreturn\u001b[39;00m function(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39margs)\n",
      "File \u001b[0;32m~/miniconda/lib/python3.11/site-packages/awswrangler/athena/_executions.py:236\u001b[0m, in \u001b[0;36mwait_query\u001b[0;34m(query_execution_id, boto3_session, athena_query_wait_polling_delay)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[39mif\u001b[39;00m state \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mFAILED\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 236\u001b[0m     \u001b[39mraise\u001b[39;00m exceptions\u001b[39m.\u001b[39mQueryFailed(response[\u001b[39m\"\u001b[39m\u001b[39mStatus\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mStateChangeReason\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m    237\u001b[0m \u001b[39mif\u001b[39;00m state \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mCANCELLED\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "\u001b[0;31mQueryFailed\u001b[0m: Insufficient permissions to execute the query. Insufficient Lake Formation permission(s) on presentation_zone . You may need to manually clean the data at location 's3://de-c3w2a1-883041721666-us-east-1-athena-wg/tables/1f7cfb24-abaa-49c5-a458-a54982f79225' before retrying. Athena will not delete data in your account.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAccessDeniedException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m sql \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mSELECT * FROM ratings_for_ml LIMIT 10;\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m df \u001b[39m=\u001b[39m wr\u001b[39m.\u001b[39mathena\u001b[39m.\u001b[39mread_sql_query(\n\u001b[1;32m      3\u001b[0m     sql, \n\u001b[1;32m      4\u001b[0m     database\u001b[39m=\u001b[39mPRESENTATION_DATABASE_NAME, \n\u001b[1;32m      5\u001b[0m     workgroup\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mde-c3w2a1-workgroup\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[1;32m      6\u001b[0m     boto3_session\u001b[39m=\u001b[39msession)\n\u001b[1;32m      7\u001b[0m df\u001b[39m.\u001b[39mhead()\n",
      "File \u001b[0;32m~/miniconda/lib/python3.11/site-packages/awswrangler/_config.py:735\u001b[0m, in \u001b[0;36mapply_configs.<locals>.wrapper\u001b[0;34m(*args_raw, **kwargs)\u001b[0m\n\u001b[1;32m    733\u001b[0m         \u001b[39mdel\u001b[39;00m args[name]\n\u001b[1;32m    734\u001b[0m         args \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkeywords}\n\u001b[0;32m--> 735\u001b[0m \u001b[39mreturn\u001b[39;00m function(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39margs)\n",
      "File \u001b[0;32m~/miniconda/lib/python3.11/site-packages/awswrangler/_utils.py:178\u001b[0m, in \u001b[0;36mvalidate_kwargs.<locals>.decorator.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[39mif\u001b[39;00m condition_fn() \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(passed_unsupported_kwargs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    176\u001b[0m     \u001b[39mraise\u001b[39;00m exceptions\u001b[39m.\u001b[39mInvalidArgument(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmessage\u001b[39m}\u001b[39;00m\u001b[39m `\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(passed_unsupported_kwargs)\u001b[39m}\u001b[39;00m\u001b[39m`.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 178\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda/lib/python3.11/site-packages/awswrangler/athena/_read.py:1080\u001b[0m, in \u001b[0;36mread_sql_query\u001b[0;34m(sql, database, ctas_approach, unload_approach, ctas_parameters, unload_parameters, categories, chunksize, s3_output, workgroup, encryption, kms_key, keep_files, use_threads, boto3_session, client_request_token, athena_cache_settings, data_source, athena_query_wait_polling_delay, params, paramstyle, dtype_backend, s3_additional_kwargs, pyarrow_additional_kwargs)\u001b[0m\n\u001b[1;32m   1077\u001b[0m ctas_bucketing_info \u001b[39m=\u001b[39m ctas_parameters\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mbucketing_info\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1078\u001b[0m ctas_write_compression \u001b[39m=\u001b[39m ctas_parameters\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mcompression\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1080\u001b[0m \u001b[39mreturn\u001b[39;00m _resolve_query_without_cache(\n\u001b[1;32m   1081\u001b[0m     sql\u001b[39m=\u001b[39msql,\n\u001b[1;32m   1082\u001b[0m     database\u001b[39m=\u001b[39mdatabase,\n\u001b[1;32m   1083\u001b[0m     data_source\u001b[39m=\u001b[39mdata_source,\n\u001b[1;32m   1084\u001b[0m     ctas_approach\u001b[39m=\u001b[39mctas_approach,\n\u001b[1;32m   1085\u001b[0m     unload_approach\u001b[39m=\u001b[39munload_approach,\n\u001b[1;32m   1086\u001b[0m     unload_parameters\u001b[39m=\u001b[39munload_parameters,\n\u001b[1;32m   1087\u001b[0m     categories\u001b[39m=\u001b[39mcategories,\n\u001b[1;32m   1088\u001b[0m     chunksize\u001b[39m=\u001b[39mchunksize,\n\u001b[1;32m   1089\u001b[0m     s3_output\u001b[39m=\u001b[39ms3_output,\n\u001b[1;32m   1090\u001b[0m     workgroup\u001b[39m=\u001b[39mworkgroup,\n\u001b[1;32m   1091\u001b[0m     encryption\u001b[39m=\u001b[39mencryption,\n\u001b[1;32m   1092\u001b[0m     kms_key\u001b[39m=\u001b[39mkms_key,\n\u001b[1;32m   1093\u001b[0m     keep_files\u001b[39m=\u001b[39mkeep_files,\n\u001b[1;32m   1094\u001b[0m     ctas_database\u001b[39m=\u001b[39mctas_database,\n\u001b[1;32m   1095\u001b[0m     ctas_temp_table_name\u001b[39m=\u001b[39mctas_temp_table_name,\n\u001b[1;32m   1096\u001b[0m     ctas_bucketing_info\u001b[39m=\u001b[39mctas_bucketing_info,\n\u001b[1;32m   1097\u001b[0m     ctas_write_compression\u001b[39m=\u001b[39mctas_write_compression,\n\u001b[1;32m   1098\u001b[0m     athena_query_wait_polling_delay\u001b[39m=\u001b[39mathena_query_wait_polling_delay,\n\u001b[1;32m   1099\u001b[0m     use_threads\u001b[39m=\u001b[39muse_threads,\n\u001b[1;32m   1100\u001b[0m     s3_additional_kwargs\u001b[39m=\u001b[39ms3_additional_kwargs,\n\u001b[1;32m   1101\u001b[0m     boto3_session\u001b[39m=\u001b[39mboto3_session,\n\u001b[1;32m   1102\u001b[0m     pyarrow_additional_kwargs\u001b[39m=\u001b[39mpyarrow_additional_kwargs,\n\u001b[1;32m   1103\u001b[0m     execution_params\u001b[39m=\u001b[39mexecution_params,\n\u001b[1;32m   1104\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[1;32m   1105\u001b[0m     client_request_token\u001b[39m=\u001b[39mclient_request_token,\n\u001b[1;32m   1106\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda/lib/python3.11/site-packages/awswrangler/athena/_read.py:530\u001b[0m, in \u001b[0;36m_resolve_query_without_cache\u001b[0;34m(sql, database, data_source, ctas_approach, unload_approach, unload_parameters, categories, chunksize, s3_output, workgroup, encryption, kms_key, keep_files, ctas_database, ctas_temp_table_name, ctas_bucketing_info, ctas_write_compression, athena_query_wait_polling_delay, use_threads, s3_additional_kwargs, boto3_session, pyarrow_additional_kwargs, execution_params, dtype_backend, client_request_token)\u001b[0m\n\u001b[1;32m    506\u001b[0m         \u001b[39mreturn\u001b[39;00m _resolve_query_without_cache_ctas(\n\u001b[1;32m    507\u001b[0m             sql\u001b[39m=\u001b[39msql,\n\u001b[1;32m    508\u001b[0m             database\u001b[39m=\u001b[39mdatabase,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    527\u001b[0m             dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[1;32m    528\u001b[0m         )\n\u001b[1;32m    529\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m--> 530\u001b[0m         catalog\u001b[39m.\u001b[39mdelete_table_if_exists(database\u001b[39m=\u001b[39mctas_database \u001b[39mor\u001b[39;00m database, table\u001b[39m=\u001b[39mname, boto3_session\u001b[39m=\u001b[39mboto3_session)\n\u001b[1;32m    531\u001b[0m \u001b[39melif\u001b[39;00m unload_approach \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    532\u001b[0m     \u001b[39mif\u001b[39;00m unload_parameters \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda/lib/python3.11/site-packages/awswrangler/_config.py:735\u001b[0m, in \u001b[0;36mapply_configs.<locals>.wrapper\u001b[0;34m(*args_raw, **kwargs)\u001b[0m\n\u001b[1;32m    733\u001b[0m         \u001b[39mdel\u001b[39;00m args[name]\n\u001b[1;32m    734\u001b[0m         args \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkeywords}\n\u001b[0;32m--> 735\u001b[0m \u001b[39mreturn\u001b[39;00m function(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39margs)\n",
      "File \u001b[0;32m~/miniconda/lib/python3.11/site-packages/awswrangler/catalog/_delete.py:89\u001b[0m, in \u001b[0;36mdelete_table_if_exists\u001b[0;34m(database, table, transaction_id, catalog_id, boto3_session)\u001b[0m\n\u001b[1;32m     87\u001b[0m client_glue \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mclient(service_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mglue\u001b[39m\u001b[39m\"\u001b[39m, session\u001b[39m=\u001b[39mboto3_session)\n\u001b[1;32m     88\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 89\u001b[0m     client_glue\u001b[39m.\u001b[39mdelete_table(\n\u001b[1;32m     90\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m_catalog_id(\n\u001b[1;32m     91\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m_transaction_id(\n\u001b[1;32m     92\u001b[0m                 transaction_id\u001b[39m=\u001b[39mtransaction_id, DatabaseName\u001b[39m=\u001b[39mdatabase, Name\u001b[39m=\u001b[39mtable, catalog_id\u001b[39m=\u001b[39mcatalog_id\n\u001b[1;32m     93\u001b[0m             )\n\u001b[1;32m     94\u001b[0m         )\n\u001b[1;32m     95\u001b[0m     )\n\u001b[1;32m     96\u001b[0m     _logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mDeleted catalog table: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, table)\n\u001b[1;32m     97\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/lib/python3.11/site-packages/botocore/client.py:553\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    550\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpy_operation_name\u001b[39m}\u001b[39;00m\u001b[39m() only accepts keyword arguments.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    551\u001b[0m     )\n\u001b[1;32m    552\u001b[0m \u001b[39m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 553\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_api_call(operation_name, kwargs)\n",
      "File \u001b[0;32m~/miniconda/lib/python3.11/site-packages/botocore/client.py:1009\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m   1005\u001b[0m     error_code \u001b[39m=\u001b[39m error_info\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mQueryErrorCode\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m error_info\u001b[39m.\u001b[39mget(\n\u001b[1;32m   1006\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCode\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1007\u001b[0m     )\n\u001b[1;32m   1008\u001b[0m     error_class \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m-> 1009\u001b[0m     \u001b[39mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m   1010\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1011\u001b[0m     \u001b[39mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mAccessDeniedException\u001b[0m: An error occurred (AccessDeniedException) when calling the DeleteTable operation: Insufficient Lake Formation permission(s): Required Drop on temp_table_d847352caa504a3cbaed6b5b2cbeed6c"
     ]
    }
   ],
   "source": [
    "sql = 'SELECT * FROM ratings_for_ml LIMIT 10;'\n",
    "df = wr.athena.read_sql_query(\n",
    "    sql, \n",
    "    database=PRESENTATION_DATABASE_NAME, \n",
    "    workgroup='de-c3w2a1-workgroup', \n",
    "    boto3_session=session)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you will provide access to the `presentation_zone` database and the `ratings_for_ml` table. For that, you require the `arn` of the `ml_data_lake_user`. Execute the following command with the corresponding user name to grab its arn. Save this value in the `ml_arn` variable in the subsequent cell replacing the placeholder `<ML_USER_ARN>` and execute it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"User\": {\n",
      "        \"Path\": \"/\",\n",
      "        \"UserName\": \"ml_data_lake_user\",\n",
      "        \"UserId\": \"AIDA43GLDLVBGU2D3KOAS\",\n",
      "        \"Arn\": \"arn:aws:iam::883041721666:user/ml_data_lake_user\",\n",
      "        \"CreateDate\": \"2025-11-06T11:19:39+00:00\",\n",
      "        \"Tags\": [\n",
      "            {\n",
      "                \"Key\": \"cloudlab\",\n",
      "                \"Value\": \"c62825a3581682l12508574t1w883041721666\"\n",
      "            }\n",
      "        ]\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!aws iam get-user --user-name ml_data_lake_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Grant database presentation_zone access to:arn:aws:iam::883041721666:user/ml_data_lake_user\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ml_arn = 'arn:aws:iam::883041721666:user/ml_data_lake_user'\n",
    "lf_utils.grant_database_access(lf_client, ml_arn, PRESENTATION_DATABASE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, although the user has access to the `presentation_zone` database, it wouldn't be able to query the `ratings_for_ml` table directly. You have to provide direct access to that table using the `lf_utils.grant_table_access()` function. Remember that the `ml_data_lake_user` should be able to only read the data. Pass the table name and the list ['SELECT'] to the function below and execute it. After that, run the query again to check that the user has the appropriate access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Grant table ratings_for_ml access to:arn:aws:iam::883041721666:user/ml_data_lake_user\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lf_utils.grant_table_access(lf_client, ml_arn, PRESENTATION_DATABASE_NAME, 'ratings_for_ml', ['SELECT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:awswrangler.athena._utils:Created CTAS table \"presentation_zone\".\"temp_table_f286e1f921b14a34b3d1dac71fdd9b65\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customernumber</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>postalcode</th>\n",
       "      <th>country</th>\n",
       "      <th>creditlimit</th>\n",
       "      <th>productcode</th>\n",
       "      <th>productline</th>\n",
       "      <th>productscale</th>\n",
       "      <th>quantityinstock</th>\n",
       "      <th>buyprice</th>\n",
       "      <th>msrp</th>\n",
       "      <th>productrating</th>\n",
       "      <th>process_ts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>114</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>3004</td>\n",
       "      <td>Australia</td>\n",
       "      <td>117300</td>\n",
       "      <td>S12_1666</td>\n",
       "      <td>Trucks and Buses</td>\n",
       "      <td>1:12</td>\n",
       "      <td>1579</td>\n",
       "      <td>78</td>\n",
       "      <td>137</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-11-06 12:27:06.564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>114</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>3004</td>\n",
       "      <td>Australia</td>\n",
       "      <td>117300</td>\n",
       "      <td>S18_1889</td>\n",
       "      <td>Classic Cars</td>\n",
       "      <td>1:18</td>\n",
       "      <td>8826</td>\n",
       "      <td>54</td>\n",
       "      <td>77</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-11-06 12:27:06.564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>114</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>3004</td>\n",
       "      <td>Australia</td>\n",
       "      <td>117300</td>\n",
       "      <td>S18_2625</td>\n",
       "      <td>Motorcycles</td>\n",
       "      <td>1:18</td>\n",
       "      <td>4357</td>\n",
       "      <td>24</td>\n",
       "      <td>61</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-11-06 12:27:06.564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>114</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>3004</td>\n",
       "      <td>Australia</td>\n",
       "      <td>117300</td>\n",
       "      <td>S18_3320</td>\n",
       "      <td>Vintage Cars</td>\n",
       "      <td>1:18</td>\n",
       "      <td>7913</td>\n",
       "      <td>58</td>\n",
       "      <td>99</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-11-06 12:27:06.564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>114</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>3004</td>\n",
       "      <td>Australia</td>\n",
       "      <td>117300</td>\n",
       "      <td>S24_1785</td>\n",
       "      <td>Planes</td>\n",
       "      <td>1:24</td>\n",
       "      <td>3627</td>\n",
       "      <td>67</td>\n",
       "      <td>109</td>\n",
       "      <td>4</td>\n",
       "      <td>2025-11-06 12:27:06.564</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customernumber       city     state postalcode    country creditlimit  \\\n",
       "0             114  Melbourne  Victoria       3004  Australia      117300   \n",
       "1             114  Melbourne  Victoria       3004  Australia      117300   \n",
       "2             114  Melbourne  Victoria       3004  Australia      117300   \n",
       "3             114  Melbourne  Victoria       3004  Australia      117300   \n",
       "4             114  Melbourne  Victoria       3004  Australia      117300   \n",
       "\n",
       "  productcode       productline productscale  quantityinstock buyprice msrp  \\\n",
       "0    S12_1666  Trucks and Buses         1:12             1579       78  137   \n",
       "1    S18_1889      Classic Cars         1:18             8826       54   77   \n",
       "2    S18_2625       Motorcycles         1:18             4357       24   61   \n",
       "3    S18_3320      Vintage Cars         1:18             7913       58   99   \n",
       "4    S24_1785            Planes         1:24             3627       67  109   \n",
       "\n",
       "   productrating               process_ts  \n",
       "0              5  2025-11-06 12:27:06.564  \n",
       "1              5  2025-11-06 12:27:06.564  \n",
       "2              2  2025-11-06 12:27:06.564  \n",
       "3              5  2025-11-06 12:27:06.564  \n",
       "4              4  2025-11-06 12:27:06.564  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session = boto3.Session(aws_access_key_id = crendentials['ACCESS_KEY'], aws_secret_access_key = crendentials['SECRET_KEY'])\n",
    "\n",
    "sql = 'SELECT * FROM ratings_for_ml LIMIT 10;'\n",
    "df = wr.athena.read_sql_query(\n",
    "    sql, \n",
    "    database=PRESENTATION_DATABASE_NAME, \n",
    "    workgroup='de-c3w2a1-workgroup', \n",
    "    boto3_session=session)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final health check, you can run the following query with another table from the `presentation_zone` database to check that the user does not have read access to it. Change the placeholder `<PRESENTATION_LAYER_TABLE>` with the table name that you want to choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:awswrangler.athena._utils:Created CTAS table \"presentation_zone\".\"temp_table_bda2a825853a41ca8f96f8ab42c6398b\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customernumber</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>postalcode</th>\n",
       "      <th>country</th>\n",
       "      <th>creditlimit</th>\n",
       "      <th>productcode</th>\n",
       "      <th>productline</th>\n",
       "      <th>productscale</th>\n",
       "      <th>quantityinstock</th>\n",
       "      <th>buyprice</th>\n",
       "      <th>msrp</th>\n",
       "      <th>productrating</th>\n",
       "      <th>process_ts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>114</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>3004</td>\n",
       "      <td>Australia</td>\n",
       "      <td>117300</td>\n",
       "      <td>S12_1666</td>\n",
       "      <td>Trucks and Buses</td>\n",
       "      <td>1:12</td>\n",
       "      <td>1579</td>\n",
       "      <td>78</td>\n",
       "      <td>137</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-11-06 12:27:06.564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>114</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>3004</td>\n",
       "      <td>Australia</td>\n",
       "      <td>117300</td>\n",
       "      <td>S18_1889</td>\n",
       "      <td>Classic Cars</td>\n",
       "      <td>1:18</td>\n",
       "      <td>8826</td>\n",
       "      <td>54</td>\n",
       "      <td>77</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-11-06 12:27:06.564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>114</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>3004</td>\n",
       "      <td>Australia</td>\n",
       "      <td>117300</td>\n",
       "      <td>S18_2625</td>\n",
       "      <td>Motorcycles</td>\n",
       "      <td>1:18</td>\n",
       "      <td>4357</td>\n",
       "      <td>24</td>\n",
       "      <td>61</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-11-06 12:27:06.564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>114</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>3004</td>\n",
       "      <td>Australia</td>\n",
       "      <td>117300</td>\n",
       "      <td>S18_3320</td>\n",
       "      <td>Vintage Cars</td>\n",
       "      <td>1:18</td>\n",
       "      <td>7913</td>\n",
       "      <td>58</td>\n",
       "      <td>99</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-11-06 12:27:06.564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>114</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>3004</td>\n",
       "      <td>Australia</td>\n",
       "      <td>117300</td>\n",
       "      <td>S24_1785</td>\n",
       "      <td>Planes</td>\n",
       "      <td>1:24</td>\n",
       "      <td>3627</td>\n",
       "      <td>67</td>\n",
       "      <td>109</td>\n",
       "      <td>4</td>\n",
       "      <td>2025-11-06 12:27:06.564</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customernumber       city     state postalcode    country creditlimit  \\\n",
       "0             114  Melbourne  Victoria       3004  Australia      117300   \n",
       "1             114  Melbourne  Victoria       3004  Australia      117300   \n",
       "2             114  Melbourne  Victoria       3004  Australia      117300   \n",
       "3             114  Melbourne  Victoria       3004  Australia      117300   \n",
       "4             114  Melbourne  Victoria       3004  Australia      117300   \n",
       "\n",
       "  productcode       productline productscale  quantityinstock buyprice msrp  \\\n",
       "0    S12_1666  Trucks and Buses         1:12             1579       78  137   \n",
       "1    S18_1889      Classic Cars         1:18             8826       54   77   \n",
       "2    S18_2625       Motorcycles         1:18             4357       24   61   \n",
       "3    S18_3320      Vintage Cars         1:18             7913       58   99   \n",
       "4    S24_1785            Planes         1:24             3627       67  109   \n",
       "\n",
       "   productrating               process_ts  \n",
       "0              5  2025-11-06 12:27:06.564  \n",
       "1              5  2025-11-06 12:27:06.564  \n",
       "2              2  2025-11-06 12:27:06.564  \n",
       "3              5  2025-11-06 12:27:06.564  \n",
       "4              4  2025-11-06 12:27:06.564  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql = 'SELECT * FROM ratings_for_ml LIMIT 10;'\n",
    "df = wr.athena.read_sql_query(\n",
    "    sql, \n",
    "    database=PRESENTATION_DATABASE_NAME, \n",
    "    workgroup='de-c3w2a1-workgroup', \n",
    "    boto3_session=session)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The medallion architecture offers several advantages for organizing and managing a data lakehouse effectively. This architecture enables the decoupling of storage and computing, allowing users to scale each layer independently based on their requirements. With this separation, organizations can optimize costs by utilizing cost-effective storage solutions while leveraging powerful computing resources for data processing and analytics tasks. In this particular case, you leveraged Amazon S3 as your data lake and storage solution and Amazon Glue for the computing required to transform the data, at the end you used Athena to create the business objects required by the organization to perform crucial analytics.\n",
    "\n",
    "Lakeformation, a service provided by AWS, complements the medallion architecture by offering robust tools for securing and managing access to data within the data lake. Data governance is crucial for ensuring data integrity, compliance, and security, and Lakeformation plays a pivotal role by providing tools and capabilities to enforce governance policies effectively across a data lakehouse environment.\n",
    "\n",
    "Please submit the lab by clicking on **Submit assignment** (top right corner)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='8'></a>\n",
    "## 8 - Enviroment Clean Up\n",
    "\n",
    "*Note*: **This section is required only if you want to rerun the Terraform commands. To receive a proper grade for this lab, please make sure that you have submitted the lab by clicking on `Submit assignment` (top right corner) before performing the clean up steps outlined in this section.**\n",
    "\n",
    "There might be cases when you will want to rerun Terraform commands. To avoid any issues with the already created resources, it is recommended to clean up environment before that. To start the cleanup, run the following command inside the `terraform` folder:\n",
    "\n",
    "```bash\n",
    "terraform destroy\n",
    "```\n",
    "\n",
    "As you have seen in this lab, AWS LakeFormation manages access to several data locations, at the end of the lab you need to restore the LakeFormation permissions to the default and revoke all the granted permissions. For this, you will call a Lambda function to perform the reset operation, use the following command in the terminal:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "aws lambda invoke --function-name de-c3w2a1-Lambda-Setup --cli-binary-format raw-in-base64-out --payload '{ \"RequestType\": \"Delete\" }' --invocation-type Event response.json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the status of your execution by using the following steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First, get the log stream name with the following command:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "aws logs describe-log-streams --log-group-name '/aws/lambda/de-c3w2a1-Lambda-Setup' --query logStreams[*].logStreamName\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This will return you the stream names associated with your execution. Copy the last one and paste it in the following command replacing the `<LOG-STREAM-NAME>` placeholder:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "aws logs get-log-events --log-group-name '/aws/lambda/de-c3w2a1-Lambda-Setup' --log-stream-name '<LOG-STREAM-NAME>'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You will be able to inspect the content of your log stream. The last part of the message should look similar to the following output:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```json\n",
    "    {\n",
    "        \"timestamp\": 1716912975732,\n",
    "        \"message\": \"END RequestId: 8dc59dd1-cbca-4c2a-912b-3bcfc5049962\\n\",\n",
    "        \"ingestionTime\": 1716912984756\n",
    "    },\n",
    "    {\n",
    "        \"timestamp\": 1716912975732,\n",
    "        \"message\": \"REPORT RequestId: 8dc59dd1-cbca-4c2a-912b-3bcfc5049962\\tDuration: 22984.81 ms\\tBilled Duration: 22985 ms\\tMemory Size: 128 MB\\tMax Memory Used: 92 MB\\t\\n\",\n",
    "        \"ingestionTime\": 1716912984756\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you see the `END` of the request and the corresponding `REPORT`, that means that your lambda has finished. While you wait for your lambda execution to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
